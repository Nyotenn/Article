# 归纳偏置

## 一、weight decay

> [**Weight decay**（权重衰减）是一种用于解决过拟合问题的正则化方法](https://zhuanlan.zhihu.com/p/607909453)[1](https://zhuanlan.zhihu.com/p/607909453)[。它通过在损失函数中添加L2正则化项来限制模型的权重](https://zhuanlan.zhihu.com/p/607909453)[1](https://zhuanlan.zhihu.com/p/607909453)[。权重衰减本质上是一个L2正则化系数，它会使模型的权重趋近于0](https://zhuanlan.zhihu.com/p/607909453)[1](https://zhuanlan.zhihu.com/p/607909453)[。这样做的好处是可以防止模型过度拟合，即使在训练数据中存在噪声或不相关的特征时也能提高模型的泛化能力](https://zhuanlan.zhihu.com/p/607909453)[1](https://zhuanlan.zhihu.com/p/607909453)。
>
> [在深度学习中，权重衰减通常通过在优化器中设置`weight_decay`参数来实现](https://zhuanlan.zhihu.com/p/607909453)[1](https://zhuanlan.zhihu.com/p/607909453)[。一般建议将`weight_decay`设置为0.0001到0.001之间的值](https://zhuanlan.zhihu.com/p/607909453)[1](https://zhuanlan.zhihu.com/p/607909453)[。较小的`weight_decay`值可以帮助保持权重较小，避免梯度爆炸，并防止模型过拟合](https://zhuanlan.zhihu.com/p/607909453)[1](https://zhuanlan.zhihu.com/p/607909453)。

## 二、label smoothing

> [**Label smoothing**（标签平滑）是一种用于防止过拟合的正则化技术](https://zhuanlan.zhihu.com/p/116466239)[1](https://zhuanlan.zhihu.com/p/116466239)[。在多分类任务中，神经网络会输出一个当前数据对应于各个类别的置信度分数，将这些分数通过softmax进行归一化处理，最终会得到当前数据属于每个类别的概率。然后计算交叉熵损失函数，最小化预测概率和标签真实概率之间的交叉熵，从而得到最优的预测概率分布](https://zhuanlan.zhihu.com/p/116466239)[1](https://zhuanlan.zhihu.com/p/116466239)。
>
> [然而，在训练数据较少、不足以表征所有样本特征的情况下，神经网络会促使自身往正确标签和错误标签差值最大的方向学习，导致网络过拟合](https://zhuanlan.zhihu.com/p/116466239)[1](https://zhuanlan.zhihu.com/p/116466239). [为了解决这个问题，可以使用标签平滑技术。标签平滑是一种正则化策略，主要是通过soft one-hot来加入噪声，减少了真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果](https://zhuanlan.zhihu.com/p/116466239)[1](https://zhuanlan.zhihu.com/p/116466239)。
>
> 在训练时，标签平滑可以通过修改目标分布来实现。具体地说，对于一个样本x和其对应的真实标签y，我们可以将其目标分布修改为：
> $$
> q(y)=(1-\epsilon)p(y)+\frac{\epsilon}{K}
> $$
> 其中 `p(y)` 是one-hot形式的真实标签分布，`K` 是类别数目，`ε` [是一个小于1的超参数。这样做可以使得模型更加鲁棒，并且可以提高模型在测试集上的性能](https://zhuanlan.zhihu.com/p/116466239)[1](https://zhuanlan.zhihu.com/p/116466239)。