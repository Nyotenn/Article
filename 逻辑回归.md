# 逻辑回归

## 一、Logistic 回归

Logistic Regression 虽然被称为回归，但其实际上是分类模型，并常用于二分类。

以二分类为例，对于所给数据集假设存在这样的一条直线可以将数据完成线性可分。

![img](https://typora-img-1255584863.cos.ap-guangzhou.myqcloud.com/202301072307464.jpeg)

决策边界可以表示为
$$
w_1x_1+w_2x_2+b=0
$$
假设某个样本点 
$$
h_w(x)=w_1x_1+w_2x_2+b>0
$$
那么可以判断它的类别为`1`。

最理想的是单位阶跃函数：
$$
p(y=1|x)=\begin{cases} 0,&z<0 \\ 0.5,&z=0&&&z=w^Tx+b \\ 1,&z>0 \end{cases}
$$
但是这个阶跃函数不可微，对数几率函数是一个常用的替代函数：
$$
y=\frac{1}{1+e^{−(w^Tx+b)}}
$$
于是有：
$$
ln\frac{y}{1−y}=w^Tx+b
$$
我们将 y 视为 x 为正例的概率，则 1-y 为 x 为其反例的概率。也就是说，当 `z` 的值越接近正无穷，概率值就越接近 `1`。

## 二、代价函数

### 1、最大似然估计

逻辑回归模型的数学形式确定后，剩下就是如何去求解模型中的参数。

设：
$$
\begin{aligned}
&P(Y=1|x)=p(x)\\
&P(Y=0|x)=1−p(x)
\end{aligned}
$$
似然函数：
$$
L(w,b)=∏[p(x_i)]^{y_i}[1−p(x_i)]^{1−y_i}
$$
其中，$x_i$ 、$y_i$ 为实际观测到的数据，式中未知的只有 $w$ 、$b$ 。利用最大似然估计，实际问题就变成了**求 w、b 在取什么值的时候，L(w)取得最大值**。

为了更方便求解，我们对等式两边同取对数，写成对数似然函数：
$$
\begin{aligned}
lnL(w,b)&=∑[y_ilnp(x_i)+(1−y_i)ln(1−p(x_i))]\\
	&=∑[y_iln\frac{p(x_i)}{1−p(x_i)}+ln(1−p(x_i))]\\
	&=∑[y_i(w⋅x_i)−ln(1+e^{w⋅x_i})]
\end{aligned}
$$
### 2、对数损失函数

若逻辑回归采取类似线性回归的损失函数，则有：
$$
Q = \sum_{1}^{n}(y_i-\frac{1}{1+e^{-(w^Tx_i+b)}})^2
$$
遗憾的是，这个函数不是凸函数，不易优化，容易陷入局部最小值。所以逻辑函数使用其他形式的函数作为损失函数，称为`对数损失函数(log loss function)`：
$$
\begin{aligned}
J(w,b) &= -\frac{1}{n}lnL(w,b)\\ 
	 &= -\frac{1}{n}\sum_{1}^{n}[y_ilnp(y_i) + (1 - y_i) ln(1 - p(y_i))]
\end{aligned}
$$
显而易见的是，对数损失函数就是对数似然函数的相反数。这很好理解。

当 $y_i$ 为 1 时，$p(x_i)$ 越大，误差越小。同样的，当 $y_i$ 为 0 时，$p(x_i)$ 越小，误差越小。由此看来，对数似然函数的绝对值即可很好地拟合损失函数，因此，对其取反即可。

## 三、求解

### 1、梯度下降

梯度下降的关键在于求出损失函数的导数：
$$
\begin{aligned}
&\frac{\partial J(w,b)}{\partial w_j} = \frac{1}{n} \sum_{i=1}^{n}[f(x_i) - y_i]x_i \\
&\frac{\partial J(w,b)}{\partial b} = \frac{1}{n} \sum_{i=1}^{n}[f(x_i) - y_i]
\end{aligned}
$$
明确导数后，采用类似线性回归中的梯度下降算法即可。

## 四、正则化

为防止过拟合，我们对代价函数增加一个限制条件，**限制其较高次的参数大小不能过大**，采用正则化对参数进行约束：
$$
J(w,b) = -\frac{1}{n}\sum_{i=1}^{n}[y_ilnp(x_i) + (1-y_i)ln(1-p(x_i))] + \frac{\lambda}{2n}\sum_{i=1}^{n}w^2
$$

$$
\begin{aligned}
&\frac{\partial J(w,b)}{\partial w_j} = \frac{1}{n} \sum_{i=1}^{n}[f(x_i) - y_i]x_i + \frac{\lambda}{n}w \\
&\frac{\partial J(w,b)}{\partial b} = \frac{1}{n} \sum_{i=1}^{n}[f(x_i) - y_i]
\end{aligned}
$$

